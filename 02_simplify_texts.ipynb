{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simplifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from llama_cpp import Llama\n",
    "from _streamlit_app.utils_prompts import (\n",
    "    SYSTEM_MESSAGE_EASIER,\n",
    "    SYSTEM_MESSAGE_LS,\n",
    "    SYSTEM_MESSAGE_ES,\n",
    "    REWRITE_COMPLETE,\n",
    "    RULES_EASIER,\n",
    "    RULES_ES,\n",
    "    RULES_LS,\n",
    "    OPENAI_TEMPLATE_EASIER,\n",
    "    OPENAI_TEMPLATE_ES,\n",
    "    OPENAI_TEMPLATE_LS,\n",
    ")\n",
    "\n",
    "TEMPLATES_EASIER = (\n",
    "    \"verständliche_sprache\",\n",
    "    SYSTEM_MESSAGE_EASIER,\n",
    "    RULES_EASIER,\n",
    "    OPENAI_TEMPLATE_EASIER,\n",
    ")\n",
    "TEMPLATES_ES = (\"einfache_sprache\", SYSTEM_MESSAGE_ES, RULES_ES, OPENAI_TEMPLATE_ES)\n",
    "TEMPLATES_LS = (\"leichte_sprache\", SYSTEM_MESSAGE_LS, RULES_LS, OPENAI_TEMPLATE_LS)\n",
    "\n",
    "TEMPLATES = (TEMPLATES_EASIER, TEMPLATES_ES, TEMPLATES_LS)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "from mistralai import Mistral\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"/Volumes/1TB Home SSD/GitHub/.env_stat\")\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "TEMPERATURE_SIMPLIFICATION = 0.15\n",
    "SLEEP = 1\n",
    "\n",
    "MAX_OUTPUT_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "\n",
    "GPT4o = \"gpt-4o\"\n",
    "\n",
    "\n",
    "def call_openai(\n",
    "    prompt,\n",
    "    system_message,\n",
    "    model_id=GPT4o,\n",
    "    temperature=TEMPERATURE_SIMPLIFICATION,\n",
    "    max_tokens=MAX_OUTPUT_TOKENS,\n",
    "):\n",
    "    time.sleep(SLEEP)\n",
    "    try:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# print(call_openai(\"Was ist die Hauptstadt der Schweiz?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "SONNET = \"claude-3-5-sonnet-latest\"\n",
    "\n",
    "\n",
    "def call_anthropic(\n",
    "    prompt,\n",
    "    system_message,\n",
    "    model_id=SONNET,\n",
    "    temperature=TEMPERATURE_SIMPLIFICATION,\n",
    "    max_tokens=MAX_OUTPUT_TOKENS,\n",
    "):\n",
    "    time.sleep(SLEEP)\n",
    "    try:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=model_id,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            system=system_message,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        return message.content[0].text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# print(call_anthropic(\"Was ist die Hauptstadt der Schweiz?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest version v3 released January 2025.\n",
    "MISTRAL_SMALL_V3 = \"mistral-small-latest\"\n",
    "\n",
    "mistral_client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "\n",
    "\n",
    "def call_mistral(\n",
    "    prompt,\n",
    "    system_message,\n",
    "    model_id=MISTRAL_SMALL_V3,\n",
    "    temperature=TEMPERATURE_SIMPLIFICATION,\n",
    "    max_tokens=MAX_OUTPUT_TOKENS,\n",
    "):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    time.sleep(SLEEP)\n",
    "    try:\n",
    "        message = mistral_client.chat.complete(\n",
    "            model=model_id,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "        return message.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# print(call_mistral(\"Was ist die Hauptstadt der Schweiz?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSS models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(llm, prompt, system_message=SYSTEM_MESSAGE_EASIER):\n",
    "    output = llm.create_chat_completion(\n",
    "        temperature=TEMPERATURE_SIMPLIFICATION,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Phi-4\": \"/home/stat/huggingface_models/Phi-4-Q6_K_L.gguf\",\n",
    "    \"Phi-4-Unsloth\": \"/home/stat/huggingface_models/Phi-4-Q5_K_M_Unsloth.gguf\",\n",
    "    \"Gemma-2-27B\": \"/home/stat/huggingface_models/Gemma-2-27b-it-Q5_K_M.gguf\",\n",
    "    \"Qwen-2.5-32B\": \"/home/stat/huggingface_models/Qwen2.5-32B-Instruct-Q5_K_M.gguf\",\n",
    "    \"Qwen-2.5-72B\": \"/home/stat/huggingface_models/Qwen2.5-72B/Qwen2.5-72B-Instruct-Q5_K_M-00001-of-00002.gguf\",\n",
    "    \"Llama-3.2-3B\": \"/home/stat/huggingface_models/Llama-3.2-3B.gguf\",\n",
    "    \"Llama-3.1-Nemotron\": \"/home/stat/huggingface_models/Llama-3.1-Nemotron-70B/Llama-3.1-Nemotron-70B-Instruct-HF-Q5_K_M-00001-of-00002.gguf\",\n",
    "    \"Llama-3.3-70B\": \"/home/stat/huggingface_models/Llama-3.3-70B/Llama-3.3-70B-Instruct-Q5_K_M-00001-of-00002.gguf\",\n",
    "    \"Deepseek-R1-Distill_Llama_8B\": \"/home/stat/huggingface_models/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf\",\n",
    "    \"Deepseek-R1-Distill_Llama_70B\": \"/home/stat/huggingface_models/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf\",\n",
    "}\n",
    "\n",
    "df = pd.read_parquet(\"_data/testdata_50_final.parq\")\n",
    "\n",
    "# Make sure as many layers of the large models are processed on the A10 GPUs.\n",
    "# 70B+ == 70 layers with 1k context, 60 layers with 4k context.\n",
    "# Smaller models fit entirely on the GPU and we can set n_gpu_layers to -1.\n",
    "n_gpu_layers = 70\n",
    "\n",
    "# Set context size to 4k to fit user input and longest prompts.\n",
    "n_ctx = 4096\n",
    "\n",
    "for model_name, model_path in models.items():\n",
    "    print(f\"Processing with: {model_name}\")\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_ctx=n_ctx,\n",
    "        n_threads=16,  # Set according to the number of threads available.\n",
    "        flash_attn=True,  # Experimental feature.\n",
    "        verbose=True,  # Set to False for production.\n",
    "    )\n",
    "\n",
    "    # Iterate throught all three modes of simplification.\n",
    "    for language_level, system_message, rules, base_prompt in TEMPLATES:\n",
    "        # Iterate over all texts.\n",
    "        results = []\n",
    "        for idx, text in enumerate(df.source_text.values):\n",
    "            print(idx)\n",
    "            final_prompt = base_prompt.format(\n",
    "                prompt=text,\n",
    "                rules=rules,\n",
    "                completeness=REWRITE_COMPLETE,\n",
    "            )\n",
    "            response = call_llm(\n",
    "                llm,\n",
    "                final_prompt,\n",
    "                system_message=system_message,\n",
    "            )\n",
    "            response = response.strip()\n",
    "            results.append((idx, response))\n",
    "            print(response)\n",
    "            print()\n",
    "        print(results)\n",
    "        pd.DataFrame(results).to_parquet(\n",
    "            f\"testdata_50_{model_name}_{language_level}.parq\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proprietary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with: Mistral-Small-v3 and verständliche_sprache\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"_data/testdata_50_final.parq\")\n",
    "\n",
    "# for model_name in [\"GPT-4o\", \"Sonnet\", \"Mistral-Small-v3\"]:\n",
    "for model_name in [\"Mistral-Small-v3\"]:\n",
    "    # Iterate throught all three modes of simplification.\n",
    "    for language_level, system_message, rules, base_prompt in TEMPLATES:\n",
    "        if language_level != \"verständliche_sprache\":\n",
    "            continue\n",
    "        print(f\"Processing with: {model_name} and {language_level}\")\n",
    "        # Iterate over all texts.\n",
    "        texts = df.source_text.values\n",
    "        args = [\n",
    "            (\n",
    "                base_prompt.format(\n",
    "                    prompt=text,\n",
    "                    rules=rules,\n",
    "                    completeness=REWRITE_COMPLETE\n",
    "                ), system_message\n",
    "            )\n",
    "            for text in texts\n",
    "        ]\n",
    "        if model_name == \"GPT-4o\":\n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                response = list(executor.map(call_openai, *zip(*args)))\n",
    "        elif model_name == \"Sonnet\":\n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                response = list(executor.map(call_anthropic, *zip(*args)))\n",
    "        elif model_name == \"Mistral-Small-v3\":\n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                response = list(executor.map(call_mistral, *zip(*args)))\n",
    "        tmp = pd.DataFrame(response)\n",
    "\n",
    "        # Small fix to align dataframe columns to results from OSS models.\n",
    "        tmp.reset_index(inplace=True)\n",
    "        tmp.columns = [0, 1]\n",
    "        \n",
    "        tmp.to_parquet(f\"_data/_results/testdata_50_{model_name}-t15_{language_level}.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
